{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11138308,"sourceType":"datasetVersion","datasetId":6947512},{"sourceId":11138470,"sourceType":"datasetVersion","datasetId":6947627}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Current Work (03/30)\n* Load in BERT base model from Transformers\n* Build classification head as modeled in Transformers\n* Load in our datasets and tokenize\n* Fine-tune classification head on our data\n* Evaluate performance\n\n#### Sources\n\n* [HuggingFace Transformers models for PyTorch - BERT](https://github.com/huggingface/transformers/blob/94ae1ba5b55e79ba766582de8a199d8ccf24a021/src/transformers/models/bert/modeling_bert.py#L957)\n* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\n* [Word Representations: A Simple and General Method for Semi-Supervised Learning](https://www.researchgate.net/publication/220873681_Word_Representations_A_Simple_and_General_Method_for_Semi-Supervised_Learning)\n* [HuggingFace documentation for Fine Tuning Transformer for MultiLabel Text Classification](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb#scrollTo=mZ7lTlkyaG7u)","metadata":{}},{"cell_type":"code","source":"# packages\nimport csv\nimport math\nimport numpy as np\nimport torch\nfrom transformers.models.bert import BertModel\nfrom transformers.models.bert.configuration_bert import BertConfig\nfrom transformers import BertTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:56:17.159782Z","iopub.execute_input":"2025-03-30T18:56:17.160117Z","iopub.status.idle":"2025-03-30T18:56:17.164396Z","shell.execute_reply.started":"2025-03-30T18:56:17.160089Z","shell.execute_reply":"2025-03-30T18:56:17.163403Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"# constants\nDEBUG = True\ncfg = BertConfig()\nBATCH_SIZE = 16\n\nif not torch.cuda.is_available():\n    print('GPU not available, running script on CPU..')\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:56:39.301271Z","iopub.execute_input":"2025-03-30T18:56:39.301703Z","iopub.status.idle":"2025-03-30T18:56:39.307721Z","shell.execute_reply.started":"2025-03-30T18:56:39.301668Z","shell.execute_reply":"2025-03-30T18:56:39.306367Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"class SentimentDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer):\n        self.df = df\n        self.data = df.review\n        self.labels = df.sentiment\n        self.tokenizer = tokenizer\n        self.max_length = cfg.max_length\n\n        #self.classes = [\"positive\", \"negative\"]\n        self.classes = [0, 1]\n    \n    def get_num_classes(self):\n        return len(self.classes)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        data = str(self.data[idx])\n        data = \" \".join(data.split())\n\n        inputs = self.tokenizer.encode_plus(\n            data,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n\n        return {\n            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),\n            'targets': torch.tensor(self.target_transform(idx), dtype=torch.long)\n        }\n\n    def target_transform(self, idx):\n        if self.labels[idx] == 'positive':\n            #return [0, 1]\n            return 1\n        elif self.labels[idx] == 'negative':\n            #return [1, 0]\n            return 0\n        else:\n            print('ERROR')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:16:35.840635Z","iopub.execute_input":"2025-03-30T19:16:35.840934Z","iopub.status.idle":"2025-03-30T19:16:35.847771Z","shell.execute_reply.started":"2025-03-30T19:16:35.840913Z","shell.execute_reply":"2025-03-30T19:16:35.846833Z"}},"outputs":[],"execution_count":174},{"cell_type":"code","source":"# load and split data functions\nimport pandas as pd\ndef load_data_from_file(filepath):\n    print('...loading dataset from file')\n    \n    df = pd.read_csv(filepath)\n    #print(df.head())\n    df['sentiment'] = df[df.columns[0]].values.tolist()\n    new_df = df[['review', 'sentiment']].copy()\n    #print(new_df.head())\n\n    return new_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:16:37.739836Z","iopub.execute_input":"2025-03-30T19:16:37.740125Z","iopub.status.idle":"2025-03-30T19:16:37.744536Z","shell.execute_reply.started":"2025-03-30T19:16:37.740104Z","shell.execute_reply":"2025-03-30T19:16:37.743435Z"}},"outputs":[],"execution_count":175},{"cell_type":"code","source":"new_df = load_data_from_file('/kaggle/input/archeage/archeage.csv')\n\ntrain_size = 0.8\ntrain_dataset=new_df.sample(frac=train_size,random_state=200)\ntest_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(new_df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\ntrain_set = SentimentDataset(train_dataset, tokenizer)\n\ntrain_params = {'batch_size': BATCH_SIZE, 'shuffle': True}\n\ntrain_dataloader = torch.utils.data.DataLoader(train_set, **train_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:16:39.665321Z","iopub.execute_input":"2025-03-30T19:16:39.665644Z","iopub.status.idle":"2025-03-30T19:16:40.150231Z","shell.execute_reply.started":"2025-03-30T19:16:39.665621Z","shell.execute_reply":"2025-03-30T19:16:40.149333Z"}},"outputs":[{"name":"stdout","text":"...loading dataset from file\nFULL Dataset: (1718, 2)\nTRAIN Dataset: (1374, 2)\nTEST Dataset: (344, 2)\n","output_type":"stream"}],"execution_count":176},{"cell_type":"code","source":"# create dataloaders\narcheage_data, archeage_labels = load_data_from_file('/kaggle/input/archeage/archeage.csv')\nntua_data, ntua_labels = load_data_from_file('/kaggle/input/ntua-dataset/ntua.csv')\n\ndata = archeage_data + ntua_data\nlabels = archeage_labels + ntua_labels\ntrain_data, val_data, test_data, train_labels, val_labels, test_labels = split_data(data, labels)\n\ndef load_dataset(batch_size=BATCH_SIZE):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\n    train_dataset = SentimentDataset(train_data, train_labels, tokenizer)\n    val_dataset = SentimentDataset(val_data, val_labels, tokenizer)\n    test_dataset = SentimentDataset(test_data, test_labels, tokenizer)\n\n    num_labels = train_dataset.get_num_classes()\n\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n    return train_dataloader, val_dataloader, test_dataloader, num_labels\n\ntrain_dataloader, val_dataloader, test_dataloader, num_labels = load_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T18:35:22.470736Z","iopub.execute_input":"2025-03-30T18:35:22.471025Z","iopub.status.idle":"2025-03-30T18:35:22.633761Z","shell.execute_reply.started":"2025-03-30T18:35:22.471003Z","shell.execute_reply":"2025-03-30T18:35:22.632739Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"...loading dataset from file\n  sentiment                                             review\n0  positive  @archeage i appreciate the \"some few of you\" :...\n1  positive                               @archeage i love you\n2  positive  @archeage been playing for the pass 3 days and...\n3  positive  yay #archeage is working for me now... dunno a...\n4  positive  played archeage (pc); guild wars 2 (pc) and 1 ...\n                                              review sentiment\n0  @archeage i appreciate the \"some few of you\" :...  positive\n1                               @archeage i love you  positive\n2  @archeage been playing for the pass 3 days and...  positive\n3  yay #archeage is working for me now... dunno a...  positive\n4  played archeage (pc); guild wars 2 (pc) and 1 ...  positive\n...loading dataset from file\n  sentiment                                             review\n0  positive  @ddlovato hey, I just wanted to no when your d...\n1  positive  Great Morning USA! @JoeJonasLoverxo @Courtney7...\n2  positive  RT @AtlanticGolf Post a comment on our fan pag...\n3  positive  Beautifull news - \"endless music\" of Michael J...\n4  positive            Hungry? Maybe? Reading harry potter. :)\n                                              review sentiment\n0  @ddlovato hey, I just wanted to no when your d...  positive\n1  Great Morning USA! @JoeJonasLoverxo @Courtney7...  positive\n2  RT @AtlanticGolf Post a comment on our fan pag...  positive\n3  Beautifull news - \"endless music\" of Michael J...  positive\n4            Hungry? Maybe? Reading harry potter. :)  positive\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"class BERTForSentimentAnalysis(BertModel):\n    \"\"\"\n    from Bert For Sequence Classification\n    \"\"\"\n    def __init__(self, config):\n        super().__init__(config=config)\n        self.num_labels = 2\n        self.config = config\n\n        self.bert = BertModel(config)\n        self.dropout = torch.nn.Dropout(0.1)\n        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n\n        # TODO: only weights for the classifier head should show as uninitialized\n        self.post_init()\n\n    def forward(self,\n               input_ids=None,\n               attention_mask=None,\n               token_type_ids=None,\n               position_ids=None,\n               head_mask=None,\n               inputs_embeds=None,\n               labels=None,\n               output_attentions=None,\n               output_hidden_states=None,\n               return_dict=None):        \n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=False\n        )\n        \n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:15:27.185477Z","iopub.execute_input":"2025-03-30T19:15:27.185795Z","iopub.status.idle":"2025-03-30T19:15:27.191774Z","shell.execute_reply.started":"2025-03-30T19:15:27.185769Z","shell.execute_reply":"2025-03-30T19:15:27.190717Z"}},"outputs":[],"execution_count":169},{"cell_type":"code","source":"print('...creating BERT model')\nmodel = BERTForSentimentAnalysis(cfg).from_pretrained('bert-base-cased')\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:15:30.065041Z","iopub.execute_input":"2025-03-30T19:15:30.065399Z","iopub.status.idle":"2025-03-30T19:15:34.436612Z","shell.execute_reply.started":"2025-03-30T19:15:30.065370Z","shell.execute_reply":"2025-03-30T19:15:34.435655Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"...creating BERT model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BERTForSentimentAnalysis were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"BERTForSentimentAnalysis(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":170},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    #print(outputs)\n    #print(targets)\n    #return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n    return torch.nn.CrossEntropyLoss()(outputs, targets)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:17:17.820906Z","iopub.execute_input":"2025-03-30T19:17:17.821273Z","iopub.status.idle":"2025-03-30T19:17:17.828116Z","shell.execute_reply.started":"2025-03-30T19:17:17.821244Z","shell.execute_reply":"2025-03-30T19:17:17.827384Z"}},"outputs":[],"execution_count":180},{"cell_type":"code","source":"def train(model, dataloader, device, epoch):\n    model.train()\n    for _,data in enumerate(dataloader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        if _%5000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:17:19.420054Z","iopub.execute_input":"2025-03-30T19:17:19.420414Z","iopub.status.idle":"2025-03-30T19:17:19.426032Z","shell.execute_reply.started":"2025-03-30T19:17:19.420387Z","shell.execute_reply":"2025-03-30T19:17:19.425200Z"}},"outputs":[],"execution_count":181},{"cell_type":"code","source":"for epoch in range(2):\n    train(model, train_dataloader, DEVICE, epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:17:21.760357Z","iopub.execute_input":"2025-03-30T19:17:21.760704Z","iopub.status.idle":"2025-03-30T19:17:37.962370Z","shell.execute_reply.started":"2025-03-30T19:17:21.760675Z","shell.execute_reply":"2025-03-30T19:17:37.961654Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0, Loss:  0.1493157297372818\nEpoch: 1, Loss:  0.06309366226196289\n","output_type":"stream"}],"execution_count":182},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    nn.CrossEntropyLoss()(outputs, targets)\noptimizer = torch.optim.Adam(params=BERT_ins.parameters(), lr=2e-5)\n\ndef train(train_dataloader, model, device):\n    model.train()\n    #running_acc = 0.0\n    #running_loss = 0.0\n    \n    for i, (data, targets) in enumerate(train_dataloader):\n        #optimizer.zero_grad()\n        print('in train loop')\n        print(tokenizer.decode(data['input_ids'][0]))\n        print(tokenizer.decode(data['token_type_ids'][0]))\n        #input_ids = data['input_ids'].to(device)\n        #token_type_ids = data['token_type_ids'].to(device)\n        #attention_mask = data['attention_mask'].to(device)\n        input_ids = torch.tensor(data['input_ids'], dtype=torch.long)\n        token_type_ids = torch.tensor(data['token_type_ids'], dtype=torch.long)\n        attention_mask = torch.tensor(data['attention_mask'], dtype=torch.long)\n        targets = targets.to(device)\n        targets = torch.tensor(targets, dtype=torch.float)\n\n        #outputs = model(images)\n        outputs = model(input_ids=input_ids,\n                        token_type_ids=token_type_ids,\n                        attention_mask=attention_mask)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        print('loss:', loss.item())\n        #loss = criterion(outputs, )\n        #hidden_state = outputs.last_hidden_state\n        #predictions.append(torch.argmax(hidden_state, 1).cpu())\n        #predictions.append(outputs.cpu())\n\n        #loss = loss_function(outputs, labels)\n        #acc = compute_accuracy(outputs,labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        #running_loss += loss.item()\n        #running_acc += acc\n\n        #if (i+1) % 256 == 0:\n        #    print(\n        #        f'TRAINING --> Epoch: {epoch+1}/{num_epochs}, ' +\n        #        f'Step: {i+1}/{total_steps}, ' +\n        #        f'Loss: {running_loss / (i+1)}, '\n        #        f'Accuracy: {running_acc / (i+1)}'\n        #    )\n\n    #running_loss = running_loss / total_steps\n    #running_acc = running_acc / total_steps\n\n    #return running_loss, running_acc\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:45:34.400993Z","iopub.status.idle":"2025-03-29T19:45:34.401238Z","shell.execute_reply":"2025-03-29T19:45:34.401135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_best_model(\n    model: torch.nn.Module,\n    model_save_path,\n    val_loss: float,\n    val_losses: list,\n    epoch: int,\n    keep_models: bool = False\n):\n    \"\"\"Save the model if it is the first epoch. Subsequently, save the model\n    only if a lower validation loss is achieved whilst training.\n\n    :param model: The model to save.\n    :type model: torch.nn.Module\n    :param model_save_path: The location to save the model to.\n    :type model_save_path: Path\n    :param val_loss: The current epoch's validation loss.\n    :type val_loss: float\n    :param val_losses: The history of all other validation losses.\n    :type val_losses: list\n    :param epoch: The current epoch number.\n    :type epoch: int\n    :param keep_models: Should all models be saved, defaults to False\n    :type keep_models: bool, optional\n    \"\"\"\n    # Should we keep all models or just one\n    if keep_models:\n        model_save_path = model_save_path / f'model_{epoch+1}_{val_loss}.pt'\n    else:\n        model_save_path = model_save_path / f'model_state_dict3.pt'\n    # Save the first model\n    if len(val_losses) == 0:\n        torch.save(\n            model.state_dict(),\n            model_save_path\n        )\n        print(\n            'SAVING --> First epoch: \\n' +\n            f'Val Loss: {val_loss}\\n' +\n            f'Saving new model to:\\n{model_save_path}'\n        )\n    elif val_loss < min(val_losses):\n        # If our new validation loss is less than the previous best save the\n        # model\n        print(\n            'SAVING --> Found model with better validation loss: \\n' +\n            f'New Best Val Loss: {val_loss}\\n' +\n            f'Old Best Val Loss: {min(val_losses)}\\n'\n            f'Saving new model to:\\n{model_save_path}'\n        )\n        torch.save(\n            model.state_dict(),\n            model_save_path\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:45:34.399224Z","iopub.status.idle":"2025-03-29T19:45:34.399478Z","shell.execute_reply":"2025-03-29T19:45:34.399372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_accuracy(outputs, labels):\n    predictions = torch.argmax(outputs, 1)\n    num_predictions = len(predictions)\n    num_incorrect = torch.count_nonzero(predictions-labels)\n    accuracy = (num_predictions-num_incorrect)/num_predictions\n\n    return accuracy.item()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-03-29T19:45:34.400040Z","iopub.status.idle":"2025-03-29T19:45:34.400285Z","shell.execute_reply":"2025-03-29T19:45:34.400185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validation(val_dataloader: torch.utils.data.DataLoader, model: torch.nn.Module, loss_function,\n               epoch: int, num_epochs: int, total_steps: int, device: str) -> tuple:\n    running_loss = 0.0\n    running_acc = 0.0\n\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(val_dataloader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n\n            loss = loss_function(outputs, labels)\n            acc = compute_accuracy(outputs, labels)\n\n            running_loss += loss.item()\n            running_acc += acc\n\n            if (i+1) % 256 == 0:\n                print(\n                    f'VALIDATION --> Epoch: {epoch+1}/{num_epochs}, ' +\n                    f'Step: {i+1}/{total_steps}, ' +\n                    f'Val Loss: {running_loss / (i+1)}, ' +\n                    f'Val Acc: {running_acc / (i+1)}'\n                )\n    running_loss = running_loss / total_steps\n    running_acc = running_acc / total_steps\n\n    return running_loss, running_acc","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-03-29T19:45:34.401904Z","iopub.status.idle":"2025-03-29T19:45:34.402157Z","shell.execute_reply":"2025-03-29T19:45:34.402054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_loop(train_dataloader, model, loss, optimizer, num_epochs, device, model_save_path, val_dataloader):\n    print(f'Models will be saved to: {model_save_path}')\n    train_losses = []\n    train_accs = []\n    val_losses = []\n    val_accs = []\n\n    # Create the save path for the model\n    if not model_save_path.exists():\n        model_save_path.mkdir(exist_ok=True, parents=True)\n\n    # Total number batches in training set\n    train_total_steps = len(train_dataloader)\n    val_total_steps = len(val_dataloader)\n\n    # Perform training loop\n    for epoch in range(num_epochs):\n        # Enable model training\n        model.train(True)\n\n        # Enter the training function loop\n        train_loss, train_acc = train(\n            train_dataloader,\n            model,\n            loss_function,\n            optimizer,\n            epoch,\n            num_epochs,\n            train_total_steps,\n            device\n        )\n        print(\n            f'TRAINING --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n            f'Avg Loss: {train_loss}, Avg Accuracy: {train_acc}'\n        )\n\n        # Enter the validation loop\n        val_loss, val_acc = validation(\n            val_dataloader,\n            model,\n            loss_function,\n            epoch,\n            num_epochs,\n            val_total_steps,\n            device\n        )\n        print(\n            f'VALIDATION --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n            f'Avg Loss: {val_loss}, Avg Accuracy: {val_acc}'\n        )\n\n        # Determine if we should save the model\n        save_best_model(model, model_save_path, val_loss, val_losses, epoch)\n\n        # Record the stats\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n    return (train_losses, train_accs), (val_losses, val_accs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:45:34.402813Z","iopub.status.idle":"2025-03-29T19:45:34.403147Z","shell.execute_reply":"2025-03-29T19:45:34.402995Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: train and validate\nNUM_EPOCHS = 2\n# TODO: loss function and optimizer\n#loss = 0\n#opt = 0\nprint('training..')\nret = train(train_dataloader, BERT_ins, DEVICE)\nprint(ret[0])\n#(train_losses, train_accs), (val_losses, val_accs) = train_model(train_dataloader,BERT_ins,loss,opt,NUM_EPOCHS,DEVICE,model_save_path=Path('./models/BERT'),val_dataloader=val_dataloader)\n# Get the best validation loss and accuracy\n#print(f'Best Validation Loss: {min(val_losses)} after epoch {np.argmin(val_losses) + 1}')\n#print(f'Best Validation Acc: {max(val_accs)} after epoch {np.argmax(val_accs) + 1}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:45:34.404083Z","iopub.status.idle":"2025-03-29T19:45:34.404346Z","shell.execute_reply":"2025-03-29T19:45:34.404238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inference\n\"\"\"\nBERT_ins.eval()\nwith torch.no_grad():\n    predictions = []\n    \n    for i, (data) in enumerate(val_dataloader):\n        input_ids = data['input_ids'].to(DEVICE)\n        token_type_ids = data['token_type_ids'].to(DEVICE)\n        attention_mask = data['attention_mask'].to(DEVICE)\n    \n        outputs = BERT_ins(input_ids=input_ids,\n                          token_type_ids=token_type_ids,\n                          attention_mask=attention_mask)\n        hidden_state = outputs.last_hidden_state\n        predictions.append(torch.argmax(hidden_state, 1).cpu())\nprint(predictions[0])\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:45:34.405038Z","iopub.status.idle":"2025-03-29T19:45:34.405332Z","shell.execute_reply":"2025-03-29T19:45:34.405208Z"}},"outputs":[],"execution_count":null}]}