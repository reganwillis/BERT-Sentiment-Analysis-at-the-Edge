{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11138308,"sourceType":"datasetVersion","datasetId":6947512},{"sourceId":11138470,"sourceType":"datasetVersion","datasetId":6947627},{"sourceId":11219528,"sourceType":"datasetVersion","datasetId":7006607}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT For Sentiment Analysis\n\n### Current Work (03/30)\n* ~Load in BERT base model from Transformers~\n* ~Build classification head as modeled in Transformers~\n* ~Load in our datasets and tokenize~\n* Fine-tune classification head on our data\n* ~Evaluate performance on test dataset~\n* Strategies to prevent overfitting on train dataset\n\n### Data\n* [Archeage - Sentiment Analysis Datasets](https://github.com/hadis-1/Sentiment-Analysis-Datasets/blob/main/archeage.csv)\n* [Ntua - Sentiment Analysis Datasets](https://github.com/hadis-1/Sentiment-Analysis-Datasets/blob/main/ntua.csv)\n* [HCR - Sentiment Analysis Datasets](https://github.com/hadis-1/Sentiment-Analysis-Datasets/blob/main/hcr.csv)\n\n\n#### Sources\n\n* [HuggingFace Transformers models for PyTorch - BERT](https://github.com/huggingface/transformers/blob/94ae1ba5b55e79ba766582de8a199d8ccf24a021/src/transformers/models/bert/modeling_bert.py#L957)\n* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\n* [Word Representations: A Simple and General Method for Semi-Supervised Learning](https://www.researchgate.net/publication/220873681_Word_Representations_A_Simple_and_General_Method_for_Semi-Supervised_Learning)\n* [HuggingFace documentation for Fine Tuning Transformer for MultiLabel Text Classification](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb#scrollTo=mZ7lTlkyaG7u)","metadata":{}},{"cell_type":"code","source":"# packages\nimport csv\nimport math\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom transformers import BertTokenizer\nfrom transformers.models.bert import BertModel\nfrom transformers.models.bert.configuration_bert import BertConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:07.577198Z","iopub.execute_input":"2025-03-30T20:42:07.577557Z","iopub.status.idle":"2025-03-30T20:42:07.582032Z","shell.execute_reply.started":"2025-03-30T20:42:07.577527Z","shell.execute_reply":"2025-03-30T20:42:07.581133Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# constants\nDEBUG = True\ncfg = BertConfig()\nBATCH_SIZE = 16\nNUM_EPOCHS = 10\n\nif not torch.cuda.is_available():\n    print('GPU not available, running script on CPU..')\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:07.583332Z","iopub.execute_input":"2025-03-30T20:42:07.583636Z","iopub.status.idle":"2025-03-30T20:42:07.598239Z","shell.execute_reply.started":"2025-03-30T20:42:07.583606Z","shell.execute_reply":"2025-03-30T20:42:07.597467Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"class SentimentDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer):\n        self.df = df\n        self.data = df.review\n        self.labels = df.sentiment\n        self.tokenizer = tokenizer\n        self.max_length = cfg.max_length\n\n        self.classes = [0, 1]  # negative, positive\n    \n    def get_num_classes(self):\n        return len(self.classes)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        inputs = self.tokenize(idx)\n\n        return {\n            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),\n            'targets': torch.tensor(self.target_transform(idx), dtype=torch.long)\n        }\n\n    def tokenize(self, idx):\n        data = str(self.data[idx])\n        data = \" \".join(data.split())\n\n        inputs = self.tokenizer.encode_plus(\n            data,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n\n        return inputs    \n\n    def target_transform(self, idx):\n        if self.labels[idx] == 'positive':\n            return 1\n        elif self.labels[idx] == 'negative':\n            return 0\n        else:\n            print('[ERROR]: label not accepted:', self.labels[idx], 'must be positive or negative')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:07.599658Z","iopub.execute_input":"2025-03-30T20:42:07.599886Z","iopub.status.idle":"2025-03-30T20:42:07.615287Z","shell.execute_reply.started":"2025-03-30T20:42:07.599866Z","shell.execute_reply":"2025-03-30T20:42:07.614610Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# load and split data functions\ndef load_data_from_file(filepath):\n    print('...loading dataset from file')\n    \n    df = pd.read_csv(filepath)\n    df['sentiment'] = df[df.columns[0]].values.tolist()\n    new_df = df[['review', 'sentiment']].copy()\n\n    return new_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:07.616539Z","iopub.execute_input":"2025-03-30T20:42:07.616861Z","iopub.status.idle":"2025-03-30T20:42:07.635307Z","shell.execute_reply.started":"2025-03-30T20:42:07.616832Z","shell.execute_reply":"2025-03-30T20:42:07.634554Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"archeage_df = load_data_from_file('/kaggle/input/archeage/archeage.csv')\nntua_df = load_data_from_file('/kaggle/input/ntua-dataset/ntua.csv')\nhcr_df = load_data_from_file('/kaggle/input/hcr-dataset/hcr.csv')\nall_data = pd.concat([archeage_df, ntua_df, hcr_df], ignore_index=True)\n\ntrain_size = 0.7\nval_size = 0.2\n\ntrain_dataset = all_data.sample(frac=train_size,random_state=200)\nremaining = all_data.drop(train_dataset.index).reset_index(drop=True)\nval_dataset = remaining.sample(frac=train_size+val_size,random_state=200)\ntest_dataset = remaining.drop(val_dataset.index).reset_index(drop=True)\n\ntrain_dataset = train_dataset.reset_index(drop=True)\nval_dataset = val_dataset.reset_index(drop=True)\ntest_dataset = test_dataset.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(all_data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"VALIDATION Dataset: {}\".format(val_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\ntrain_dataset = SentimentDataset(train_dataset, tokenizer)\nval_dataset = SentimentDataset(val_dataset, tokenizer)\ntest_dataset = SentimentDataset(test_dataset, tokenizer)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:07.636294Z","iopub.execute_input":"2025-03-30T20:42:07.636571Z","iopub.status.idle":"2025-03-30T20:42:07.811528Z","shell.execute_reply.started":"2025-03-30T20:42:07.636550Z","shell.execute_reply":"2025-03-30T20:42:07.810564Z"}},"outputs":[{"name":"stdout","text":"...loading dataset from file\n...loading dataset from file\n...loading dataset from file\nFULL Dataset: (3904, 2)\nTRAIN Dataset: (2733, 2)\nVALIDATION Dataset: (1054, 2)\nTEST Dataset: (117, 2)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"class BERTForSentimentAnalysis(BertModel):\n    \"\"\"\n    from Bert For Sequence Classification\n    \"\"\"\n    def __init__(self, config):\n        super().__init__(config=config)\n        self.num_labels = 2\n        self.config = config\n\n        self.bert = BertModel(config)\n        self.dropout = torch.nn.Dropout(0.1)\n        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n\n        self.post_init()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,):        \n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=False\n        )\n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:07.813705Z","iopub.execute_input":"2025-03-30T20:42:07.813943Z","iopub.status.idle":"2025-03-30T20:42:07.819220Z","shell.execute_reply.started":"2025-03-30T20:42:07.813912Z","shell.execute_reply":"2025-03-30T20:42:07.818391Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"print('...creating BERT model')\nmodel = BERTForSentimentAnalysis(cfg).from_pretrained('bert-base-cased')\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:07.820227Z","iopub.execute_input":"2025-03-30T20:42:07.820582Z","iopub.status.idle":"2025-03-30T20:42:12.709577Z","shell.execute_reply.started":"2025-03-30T20:42:07.820554Z","shell.execute_reply":"2025-03-30T20:42:12.708414Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"...creating BERT model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BERTForSentimentAnalysis were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"BERTForSentimentAnalysis(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.CrossEntropyLoss()(outputs, targets)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:12.710574Z","iopub.execute_input":"2025-03-30T20:42:12.710830Z","iopub.status.idle":"2025-03-30T20:42:12.717509Z","shell.execute_reply.started":"2025-03-30T20:42:12.710796Z","shell.execute_reply":"2025-03-30T20:42:12.716675Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def compute_accuracy(outputs, targets):\n    predictions = torch.argmax(outputs, 1)\n    num_predictions = len(predictions)\n\n    predictions = predictions.cpu()\n    targets = targets.cpu()\n    num_incorrect = 0\n    for i in range(len(predictions)):\n        if not predictions[i] == targets[i]:\n            num_incorrect = num_incorrect + 1\n    accuracy = (num_predictions-num_incorrect)/num_predictions\n\n    return accuracy","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-03-30T20:42:12.718446Z","iopub.execute_input":"2025-03-30T20:42:12.718680Z","iopub.status.idle":"2025-03-30T20:42:12.736547Z","shell.execute_reply.started":"2025-03-30T20:42:12.718660Z","shell.execute_reply":"2025-03-30T20:42:12.735724Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def train(model, dataloader, device, epoch, num_epochs, total_steps):\n    running_loss = 0.0\n    running_acc = 0.0\n\n    for i,data in enumerate(dataloader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        accuracy = compute_accuracy(outputs, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        running_acc += accuracy\n\n        if (i+1) % 20 == 0:\n            print(\n                f'TRAINING --> Epoch: {epoch+1}/{num_epochs}, ' +\n                f'Step: {i+1}/{total_steps}, ' +\n                f'Loss: {running_loss / (i+1)}, '\n                f'Accuracy: {running_acc / (i+1)}'\n            )\n    \n    running_loss = running_loss / total_steps\n    running_acc = running_acc / total_steps\n\n    return running_loss, running_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:12.737390Z","iopub.execute_input":"2025-03-30T20:42:12.737678Z","iopub.status.idle":"2025-03-30T20:42:12.755990Z","shell.execute_reply.started":"2025-03-30T20:42:12.737658Z","shell.execute_reply":"2025-03-30T20:42:12.755373Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def validate(model, dataloader, device, epoch, num_epochs, total_steps):\n    running_loss = 0.0\n    running_acc = 0.0\n\n    with torch.no_grad():\n        for i,data in enumerate(dataloader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n    \n            outputs = model(ids, mask, token_type_ids)\n\n            loss = loss_fn(outputs, targets)\n            accuracy = compute_accuracy(outputs, targets)\n    \n            running_loss += loss.item()\n            running_acc += accuracy\n    \n            if (i+1) % 20 == 0:\n                print(\n                    f'VALIDATION --> Epoch: {epoch+1}/{num_epochs}, ' +\n                    f'Step: {i+1}/{total_steps}, ' +\n                    f'Loss: {running_loss / (i+1)}, '\n                    f'Accuracy: {running_acc / (i+1)}'\n                )\n    running_loss = running_loss / total_steps\n    running_acc = running_acc / total_steps\n\n    return running_loss, running_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:12.756821Z","iopub.execute_input":"2025-03-30T20:42:12.757067Z","iopub.status.idle":"2025-03-30T20:42:12.777631Z","shell.execute_reply.started":"2025-03-30T20:42:12.757047Z","shell.execute_reply":"2025-03-30T20:42:12.776949Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def save_best_model(\n    model: torch.nn.Module,\n    model_save_path,\n    val_loss: float,\n    val_losses: list,\n    epoch: int,\n    keep_models: bool = False\n):\n    \"\"\"Save the model if it is the first epoch. Subsequently, save the model\n    only if a lower validation loss is achieved whilst training.\n\n    :param model: The model to save.\n    :type model: torch.nn.Module\n    :param model_save_path: The location to save the model to.\n    :type model_save_path: Path\n    :param val_loss: The current epoch's validation loss.\n    :type val_loss: float\n    :param val_losses: The history of all other validation losses.\n    :type val_losses: list\n    :param epoch: The current epoch number.\n    :type epoch: int\n    :param keep_models: Should all models be saved, defaults to False\n    :type keep_models: bool, optional\n    \"\"\"\n    # Should we keep all models or just one\n    if keep_models:\n        model_save_path = model_save_path / f'model_{epoch+1}_{val_loss}.pt'\n    else:\n        model_save_path = model_save_path / f'model_state_dict.pt'\n    # Save the first model\n    if len(val_losses) == 0:\n        torch.save(\n            model.state_dict(),\n            model_save_path\n        )\n        print(\n            'SAVING --> First epoch: \\n' +\n            f'Val Loss: {val_loss}\\n' +\n            f'Saving new model to:\\n{model_save_path}'\n        )\n    elif val_loss < min(val_losses):\n        # If our new validation loss is less than the previous best save the\n        # model\n        print(\n            'SAVING --> Found model with better validation loss: \\n' +\n            f'New Best Val Loss: {val_loss}\\n' +\n            f'Old Best Val Loss: {min(val_losses)}\\n'\n            f'Saving new model to:\\n{model_save_path}'\n        )\n        torch.save(\n            model.state_dict(),\n            model_save_path\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:12.779583Z","iopub.execute_input":"2025-03-30T20:42:12.779804Z","iopub.status.idle":"2025-03-30T20:42:12.800707Z","shell.execute_reply.started":"2025-03-30T20:42:12.779785Z","shell.execute_reply":"2025-03-30T20:42:12.799959Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def train_loop(model, train_dataloader, val_dataloader, device, num_epochs, model_save_path=Path('./models')):\n    print(f'Models will be saved to: {model_save_path}')\n    train_losses = []\n    train_accs = []\n    val_losses = []\n    val_accs = []\n\n    if not model_save_path.exists():\n        model_save_path.mkdir(exist_ok=True, parents=True)\n\n    train_total_steps = len(train_dataloader)\n    val_total_steps = len(val_dataloader)\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss, train_accuracy = train(model, train_dataloader, device, epoch, num_epochs, train_total_steps)\n        print(\n            f'TRAINING --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n            f'Avg Loss: {train_loss}, Avg Accuracy: {train_accuracy}'\n        )\n\n        val_loss, val_accuracy = validate(model, val_dataloader, device, epoch, num_epochs, val_total_steps)\n        print(\n            f'VALIDATION --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n            f'Avg Loss: {val_loss}, Avg Accuracy: {val_accuracy}'\n        )\n\n        save_best_model(model, model_save_path, val_loss, val_losses, epoch)\n        \n        train_losses.append(train_loss)\n        train_accs.append(train_accuracy)\n        val_losses.append(val_loss)\n        val_accs.append(val_accuracy)\n    return (train_losses, train_accs), (val_losses, val_accs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:12.801891Z","iopub.execute_input":"2025-03-30T20:42:12.802258Z","iopub.status.idle":"2025-03-30T20:42:12.820203Z","shell.execute_reply.started":"2025-03-30T20:42:12.802221Z","shell.execute_reply":"2025-03-30T20:42:12.819618Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"(train_losses, train_accs), (val_losses, val_accs) = train_loop(model, train_dataloader, val_dataloader, DEVICE, NUM_EPOCHS)\nprint(f'Best Validation Loss: {min(val_losses)} after epoch {np.argmin(val_losses) + 1}')\nprint(f'Best Validation Acc: {max(val_accs)} after epoch {np.argmax(val_accs) + 1}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:12.821095Z","iopub.execute_input":"2025-03-30T20:42:12.821377Z","iopub.status.idle":"2025-03-30T20:42:54.268982Z","shell.execute_reply.started":"2025-03-30T20:42:12.821346Z","shell.execute_reply":"2025-03-30T20:42:54.267743Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Models will be saved to: models\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"TRAINING --> Epoch: 1/2, Step: 10/171, Loss: 0.7005204200744629, Accuracy: 0.49375\nTRAINING --> Epoch: 1/2, Step: 20/171, Loss: 0.6806706845760345, Accuracy: 0.54375\nTRAINING --> Epoch: 1/2, Step: 30/171, Loss: 0.6654908458391825, Accuracy: 0.5791666666666667\nTRAINING --> Epoch: 1/2, Step: 40/171, Loss: 0.6514295548200607, Accuracy: 0.6078125\nTRAINING --> Epoch: 1/2, Step: 50/171, Loss: 0.6238089364767074, Accuracy: 0.64\nTRAINING --> Epoch: 1/2, Step: 60/171, Loss: 0.6054089933633804, Accuracy: 0.6625\nTRAINING --> Epoch: 1/2, Step: 70/171, Loss: 0.5919532435280936, Accuracy: 0.6785714285714286\nTRAINING --> Epoch: 1/2, Step: 80/171, Loss: 0.5829774748533965, Accuracy: 0.68828125\nTRAINING --> Epoch: 1/2, Step: 90/171, Loss: 0.582604418694973, Accuracy: 0.6923611111111111\nTRAINING --> Epoch: 1/2, Step: 100/171, Loss: 0.5832276819646358, Accuracy: 0.693125\nTRAINING --> Epoch: 1/2, Step: 110/171, Loss: 0.5747283040122553, Accuracy: 0.7005681818181818\nTRAINING --> Epoch: 1/2, Step: 120/171, Loss: 0.5631152534236511, Accuracy: 0.7104166666666667\nTRAINING --> Epoch: 1/2, Step: 130/171, Loss: 0.5506298940915327, Accuracy: 0.7206730769230769\nTRAINING --> Epoch: 1/2, Step: 140/171, Loss: 0.5467182357396398, Accuracy: 0.7241071428571428\nTRAINING --> Epoch: 1/2, Step: 150/171, Loss: 0.5457507767279943, Accuracy: 0.7266666666666667\nTRAINING --> Epoch: 1/2, Step: 160/171, Loss: 0.5350726465694606, Accuracy: 0.7359375\nTRAINING --> Epoch: 1/2, Step: 170/171, Loss: 0.5320268945658908, Accuracy: 0.7375\nTRAINING --> Epoch 1/2 DONE, Avg Loss: 0.5308745343957031, Avg Accuracy: 0.7385852451641925\nVALIDATION --> Epoch: 1/2, Step: 10/66, Loss: 0.457596144080162, Accuracy: 0.775\nVALIDATION --> Epoch: 1/2, Step: 20/66, Loss: 0.5053109422326088, Accuracy: 0.746875\nVALIDATION --> Epoch: 1/2, Step: 30/66, Loss: 0.48426626523335775, Accuracy: 0.7645833333333333\nVALIDATION --> Epoch: 1/2, Step: 40/66, Loss: 0.479991951584816, Accuracy: 0.771875\nVALIDATION --> Epoch: 1/2, Step: 50/66, Loss: 0.4840619972348213, Accuracy: 0.77875\nVALIDATION --> Epoch: 1/2, Step: 60/66, Loss: 0.4811671031018098, Accuracy: 0.7833333333333333\nVALIDATION --> Epoch 1/2 DONE, Avg Loss: 0.4813859523697333, Avg Accuracy: 0.7824675324675325\nSAVING --> First epoch: \nVal Loss: 0.4813859523697333\nSaving new model to:\nmodels/model_state_dict.pt\nTRAINING --> Epoch: 2/2, Step: 10/171, Loss: 0.3193609341979027, Accuracy: 0.875\nTRAINING --> Epoch: 2/2, Step: 20/171, Loss: 0.34378513470292094, Accuracy: 0.859375\nTRAINING --> Epoch: 2/2, Step: 30/171, Loss: 0.3269949515660604, Accuracy: 0.8729166666666667\nTRAINING --> Epoch: 2/2, Step: 40/171, Loss: 0.31879631858319046, Accuracy: 0.871875\nTRAINING --> Epoch: 2/2, Step: 50/171, Loss: 0.32188485607504846, Accuracy: 0.8725\nTRAINING --> Epoch: 2/2, Step: 60/171, Loss: 0.3160256697485844, Accuracy: 0.8729166666666667\nTRAINING --> Epoch: 2/2, Step: 70/171, Loss: 0.3171754275049482, Accuracy: 0.86875\nTRAINING --> Epoch: 2/2, Step: 80/171, Loss: 0.3121093317866325, Accuracy: 0.86953125\nTRAINING --> Epoch: 2/2, Step: 90/171, Loss: 0.3228720749417941, Accuracy: 0.8659722222222223\nTRAINING --> Epoch: 2/2, Step: 100/171, Loss: 0.3192299937456846, Accuracy: 0.86875\nTRAINING --> Epoch: 2/2, Step: 110/171, Loss: 0.3136411020024256, Accuracy: 0.8721590909090909\nTRAINING --> Epoch: 2/2, Step: 120/171, Loss: 0.31243414704998335, Accuracy: 0.8713541666666667\nTRAINING --> Epoch: 2/2, Step: 130/171, Loss: 0.30938316056361564, Accuracy: 0.8711538461538462\nTRAINING --> Epoch: 2/2, Step: 140/171, Loss: 0.30404918209782666, Accuracy: 0.8741071428571429\nTRAINING --> Epoch: 2/2, Step: 150/171, Loss: 0.3046343617637952, Accuracy: 0.8754166666666666\nTRAINING --> Epoch: 2/2, Step: 160/171, Loss: 0.3121649638749659, Accuracy: 0.870703125\nTRAINING --> Epoch: 2/2, Step: 170/171, Loss: 0.30812063813209534, Accuracy: 0.8713235294117647\nTRAINING --> Epoch 2/2 DONE, Avg Loss: 0.30793548658577324, Avg Accuracy: 0.8711763382816013\nVALIDATION --> Epoch: 2/2, Step: 10/66, Loss: 0.35471695065498354, Accuracy: 0.84375\nVALIDATION --> Epoch: 2/2, Step: 20/66, Loss: 0.451706263422966, Accuracy: 0.8\nVALIDATION --> Epoch: 2/2, Step: 30/66, Loss: 0.4332156519095103, Accuracy: 0.8041666666666667\nVALIDATION --> Epoch: 2/2, Step: 40/66, Loss: 0.43566579930484295, Accuracy: 0.8015625\nVALIDATION --> Epoch: 2/2, Step: 50/66, Loss: 0.4531574293971062, Accuracy: 0.795\nVALIDATION --> Epoch: 2/2, Step: 60/66, Loss: 0.45371735791365303, Accuracy: 0.7927083333333333\nVALIDATION --> Epoch 2/2 DONE, Avg Loss: 0.46753886432358716, Avg Accuracy: 0.785443722943723\nSAVING --> Found model with better validation loss: \nNew Best Val Loss: 0.46753886432358716\nOld Best Val Loss: 0.4813859523697333\nSaving new model to:\nmodels/model_state_dict.pt\nBest Validation Loss: 0.46753886432358716 after epoch 2\nBest Validation Acc: 0.785443722943723 after epoch 2\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"def evaluate(model, dataloader, device, total_steps):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n\n    with torch.no_grad():\n        for i,data in enumerate(dataloader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n    \n            outputs = model(ids, mask, token_type_ids)\n\n            loss = loss_fn(outputs, targets)\n            accuracy = compute_accuracy(outputs, targets)\n    \n            running_loss += loss.item()\n            running_acc += accuracy\n    \n            if (i+1) % 256 == 0:\n                print(\n                    f'TEST' +\n                    f'Step: {i+1}/{total_steps}, ' +\n                    f'Loss: {running_loss / (i+1)}, '\n                    f'Accuracy: {running_acc / (i+1)}'\n                )\n    running_loss = running_loss / total_steps\n    running_acc = running_acc / total_steps\n\n    return running_loss, running_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:54.270041Z","iopub.execute_input":"2025-03-30T20:42:54.270400Z","iopub.status.idle":"2025-03-30T20:42:54.277056Z","shell.execute_reply.started":"2025-03-30T20:42:54.270364Z","shell.execute_reply":"2025-03-30T20:42:54.275949Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"test_loss, test_accuracy = evaluate(model, test_dataloader, DEVICE, len(test_dataloader))\nprint(\n    f'TEST --> DONE, ' +\n    f'Avg Loss: {test_loss}, Avg Accuracy: {test_accuracy}'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:42:54.278145Z","iopub.execute_input":"2025-03-30T20:42:54.278448Z","iopub.status.idle":"2025-03-30T20:42:54.579653Z","shell.execute_reply.started":"2025-03-30T20:42:54.278427Z","shell.execute_reply":"2025-03-30T20:42:54.578748Z"}},"outputs":[{"name":"stdout","text":"TEST --> DONE, Avg Loss: 0.4594200160354376, Avg Accuracy: 0.8265625\n","output_type":"stream"}],"execution_count":53}]}