{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11138308,"sourceType":"datasetVersion","datasetId":6947512},{"sourceId":11138470,"sourceType":"datasetVersion","datasetId":6947627}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Current Work (03/30)\n* Load in BERT base model from Transformers\n* Build classification head as modeled in Transformers\n* Load in our datasets and tokenize\n* Fine-tune classification head on our data\n* Evaluate performance\n\n#### Sources\n\n* [HuggingFace Transformers models for PyTorch - BERT](https://github.com/huggingface/transformers/blob/94ae1ba5b55e79ba766582de8a199d8ccf24a021/src/transformers/models/bert/modeling_bert.py#L957)\n* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\n* [Word Representations: A Simple and General Method for Semi-Supervised Learning](https://www.researchgate.net/publication/220873681_Word_Representations_A_Simple_and_General_Method_for_Semi-Supervised_Learning)\n* [HuggingFace documentation for Fine Tuning Transformer for MultiLabel Text Classification](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb#scrollTo=mZ7lTlkyaG7u)","metadata":{}},{"cell_type":"code","source":"# packages\nimport csv\nimport math\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom transformers import BertTokenizer\nfrom transformers.models.bert import BertModel\nfrom transformers.models.bert.configuration_bert import BertConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:04:07.041468Z","iopub.execute_input":"2025-03-30T20:04:07.041818Z","iopub.status.idle":"2025-03-30T20:04:07.046486Z","shell.execute_reply.started":"2025-03-30T20:04:07.041792Z","shell.execute_reply":"2025-03-30T20:04:07.045381Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":210},{"cell_type":"code","source":"# constants\nDEBUG = True\ncfg = BertConfig()\nBATCH_SIZE = 16\nNUM_EPOCHS = 2\n\nif not torch.cuda.is_available():\n    print('GPU not available, running script on CPU..')\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:52:51.800556Z","iopub.execute_input":"2025-03-30T19:52:51.800878Z","iopub.status.idle":"2025-03-30T19:52:51.805352Z","shell.execute_reply.started":"2025-03-30T19:52:51.800856Z","shell.execute_reply":"2025-03-30T19:52:51.804473Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":202},{"cell_type":"code","source":"class SentimentDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer):\n        self.df = df\n        self.data = df.review\n        self.labels = df.sentiment\n        self.tokenizer = tokenizer\n        self.max_length = cfg.max_length\n\n        self.classes = [0, 1]  # negative, positive\n    \n    def get_num_classes(self):\n        return len(self.classes)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        inputs = tokenize(idx)\n\n        return {\n            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),\n            'targets': torch.tensor(self.target_transform(idx), dtype=torch.long)\n        }\n\n    def tokenize(self, idx):\n        data = str(self.data[idx])\n        data = \" \".join(data.split())\n\n        inputs = self.tokenizer.encode_plus(\n            data,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n\n        return inputs    \n\n    def target_transform(self, idx):\n        if self.labels[idx] == 'positive':\n            return 1\n        elif self.labels[idx] == 'negative':\n            return 0\n        else:\n            print('[ERROR]: label not accepted:', self.labels[idx], 'must be positive or negative')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:24:00.595403Z","iopub.execute_input":"2025-03-30T19:24:00.595724Z","iopub.status.idle":"2025-03-30T19:24:00.603385Z","shell.execute_reply.started":"2025-03-30T19:24:00.595699Z","shell.execute_reply":"2025-03-30T19:24:00.602282Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":185},{"cell_type":"code","source":"# load and split data functions\ndef load_data_from_file(filepath):\n    print('...loading dataset from file')\n    \n    df = pd.read_csv(filepath)\n    df['sentiment'] = df[df.columns[0]].values.tolist()\n    new_df = df[['review', 'sentiment']].copy()\n\n    return new_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:24:02.640325Z","iopub.execute_input":"2025-03-30T19:24:02.640626Z","iopub.status.idle":"2025-03-30T19:24:02.645133Z","shell.execute_reply.started":"2025-03-30T19:24:02.640594Z","shell.execute_reply":"2025-03-30T19:24:02.644248Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":186},{"cell_type":"code","source":"archeage_df = load_data_from_file('/kaggle/input/archeage/archeage.csv')\nntua_df = load_data_from_file('/kaggle/input/ntua-dataset/ntua.csv')\nall_data = pd.concat([archeage_df, ntua_df], ignore_index=True)\n\ntrain_size = 0.7\nval_size = 0.2\n\ntrain_dataset = all_data.sample(frac=train_size,random_state=200)\nremaining = all_data.drop(train_dataset.index).reset_index(drop=True)\nval_dataset = remaining.sample(frac=train_size+val_size,random_state=200)\ntest_dataset = remaining.drop(val_dataset.index).reset_index(drop=True)\n\ntrain_dataset = train_dataset.reset_index(drop=True)\nval_dataset = val_dataset.reset_index(drop=True)\ntest_dataset = test_dataset.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(new_df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"VALIDATION Dataset: {}\".format(val_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\ntrain_dataset = SentimentDataset(train_dataset, tokenizer)\nval_dataset = SentimentDataset(val_dataset, tokenizer)\ntest_dataset = SentimentDataset(test_dataset, tokenizer)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:41:06.100407Z","iopub.execute_input":"2025-03-30T19:41:06.100742Z","iopub.status.idle":"2025-03-30T19:41:06.237648Z","shell.execute_reply.started":"2025-03-30T19:41:06.100714Z","shell.execute_reply":"2025-03-30T19:41:06.236935Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"...loading dataset from file\n...loading dataset from file\nFULL Dataset: (1718, 2)\nTRAIN Dataset: (1397, 2)\nVALIDATION Dataset: (539, 2)\nTEST Dataset: (60, 2)\n","output_type":"stream"}],"execution_count":196},{"cell_type":"code","source":"class BERTForSentimentAnalysis(BertModel):\n    \"\"\"\n    from Bert For Sequence Classification\n    \"\"\"\n    def __init__(self, config):\n        super().__init__(config=config)\n        self.num_labels = 2\n        self.config = config\n\n        self.bert = BertModel(config)\n        self.dropout = torch.nn.Dropout(0.1)\n        self.classifier = torch.nn.Linear(config.hidden_size, self.num_labels)\n\n        self.post_init()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,):        \n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=False\n        )\n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:42:26.241126Z","iopub.execute_input":"2025-03-30T19:42:26.241491Z","iopub.status.idle":"2025-03-30T19:42:26.247430Z","shell.execute_reply.started":"2025-03-30T19:42:26.241466Z","shell.execute_reply":"2025-03-30T19:42:26.246397Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":197},{"cell_type":"code","source":"print('...creating BERT model')\nmodel = BERTForSentimentAnalysis(cfg).from_pretrained('bert-base-cased')\nmodel.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T19:42:30.950263Z","iopub.execute_input":"2025-03-30T19:42:30.950599Z","iopub.status.idle":"2025-03-30T19:42:35.258332Z","shell.execute_reply.started":"2025-03-30T19:42:30.950556Z","shell.execute_reply":"2025-03-30T19:42:35.257234Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","text":"...creating BERT model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BERTForSentimentAnalysis were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":198,"output_type":"execute_result","data":{"text/plain":"BERTForSentimentAnalysis(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":198},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.CrossEntropyLoss()(outputs, targets)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=2e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:03:29.221263Z","iopub.execute_input":"2025-03-30T20:03:29.221686Z","iopub.status.idle":"2025-03-30T20:03:29.227981Z","shell.execute_reply.started":"2025-03-30T20:03:29.221656Z","shell.execute_reply":"2025-03-30T20:03:29.226996Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":204},{"cell_type":"code","source":"def compute_accuracy(outputs, targets):\n    predictions = torch.argmax(outputs, 1)\n    num_predictions = len(predictions)\n\n    predictions = predictions.cpu()\n    targets = targets.cpu()\n    num_incorrect = 0\n    for i in range(len(predictions)):\n        if not predictions[i] == targets[i]:\n            num_incorrect = num_incorrect + 1\n    accuracy = (num_predictions-num_incorrect)/num_predictions\n\n    return accuracy","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-03-30T20:14:16.470408Z","iopub.execute_input":"2025-03-30T20:14:16.470762Z","iopub.status.idle":"2025-03-30T20:14:16.475705Z","shell.execute_reply.started":"2025-03-30T20:14:16.470734Z","shell.execute_reply":"2025-03-30T20:14:16.474693Z"}},"outputs":[],"execution_count":235},{"cell_type":"code","source":"def train(model, dataloader, device, epoch, num_epochs, total_steps):\n    running_loss = 0.0\n    running_acc = 0.0\n\n    for i,data in enumerate(dataloader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        accuracy = compute_accuracy(outputs, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        running_acc += accuracy\n\n        if (i+1) % 256 == 0:\n            print(\n                f'TRAINING --> Epoch: {epoch+1}/{num_epochs}, ' +\n                f'Step: {i+1}/{total_steps}, ' +\n                f'Loss: {running_loss / (i+1)}, '\n                f'Accuracy: {running_acc / (i+1)}'\n            )\n    \n    running_loss = running_loss / total_steps\n    running_acc = running_acc / total_steps\n\n    return running_loss, running_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:05:17.040550Z","iopub.execute_input":"2025-03-30T20:05:17.040911Z","iopub.status.idle":"2025-03-30T20:05:17.047248Z","shell.execute_reply.started":"2025-03-30T20:05:17.040882Z","shell.execute_reply":"2025-03-30T20:05:17.046308Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":213},{"cell_type":"code","source":"def validate(model, dataloader, device, epoch, num_epochs, total_steps):\n    running_loss = 0.0\n    running_acc = 0.0\n\n    with torch.no_grad():\n        for i,data in enumerate(dataloader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n    \n            outputs = model(ids, mask, token_type_ids)\n\n            loss = loss_fn(outputs, targets)\n            accuracy = compute_accuracy(outputs, targets)\n    \n            running_loss += loss.item()\n            running_acc += accuracy\n    \n            if (i+1) % 256 == 0:\n                print(\n                    f'VALIDATION --> Epoch: {epoch+1}/{num_epochs}, ' +\n                    f'Step: {i+1}/{total_steps}, ' +\n                    f'Loss: {running_loss / (i+1)}, '\n                    f'Accuracy: {running_acc / (i+1)}'\n                )\n    running_loss = running_loss / total_steps\n    running_acc = running_acc / total_steps\n\n    return running_loss, running_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:05:19.100379Z","iopub.execute_input":"2025-03-30T20:05:19.100676Z","iopub.status.idle":"2025-03-30T20:05:19.106580Z","shell.execute_reply.started":"2025-03-30T20:05:19.100654Z","shell.execute_reply":"2025-03-30T20:05:19.105683Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":214},{"cell_type":"code","source":"def save_best_model(\n    model: torch.nn.Module,\n    model_save_path,\n    val_loss: float,\n    val_losses: list,\n    epoch: int,\n    keep_models: bool = False\n):\n    \"\"\"Save the model if it is the first epoch. Subsequently, save the model\n    only if a lower validation loss is achieved whilst training.\n\n    :param model: The model to save.\n    :type model: torch.nn.Module\n    :param model_save_path: The location to save the model to.\n    :type model_save_path: Path\n    :param val_loss: The current epoch's validation loss.\n    :type val_loss: float\n    :param val_losses: The history of all other validation losses.\n    :type val_losses: list\n    :param epoch: The current epoch number.\n    :type epoch: int\n    :param keep_models: Should all models be saved, defaults to False\n    :type keep_models: bool, optional\n    \"\"\"\n    # Should we keep all models or just one\n    if keep_models:\n        model_save_path = model_save_path / f'model_{epoch+1}_{val_loss}.pt'\n    else:\n        model_save_path = model_save_path / f'model_state_dict.pt'\n    # Save the first model\n    if len(val_losses) == 0:\n        torch.save(\n            model.state_dict(),\n            model_save_path\n        )\n        print(\n            'SAVING --> First epoch: \\n' +\n            f'Val Loss: {val_loss}\\n' +\n            f'Saving new model to:\\n{model_save_path}'\n        )\n    elif val_loss < min(val_losses):\n        # If our new validation loss is less than the previous best save the\n        # model\n        print(\n            'SAVING --> Found model with better validation loss: \\n' +\n            f'New Best Val Loss: {val_loss}\\n' +\n            f'Old Best Val Loss: {min(val_losses)}\\n'\n            f'Saving new model to:\\n{model_save_path}'\n        )\n        torch.save(\n            model.state_dict(),\n            model_save_path\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:05:21.040365Z","iopub.execute_input":"2025-03-30T20:05:21.040705Z","iopub.status.idle":"2025-03-30T20:05:21.046537Z","shell.execute_reply.started":"2025-03-30T20:05:21.040677Z","shell.execute_reply":"2025-03-30T20:05:21.045540Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":215},{"cell_type":"code","source":"def train_loop(model, train_dataloader, val_dataloader, device, num_epochs, model_save_path=Path('./models')):\n    print(f'Models will be saved to: {model_save_path}')\n    train_losses = []\n    train_accs = []\n    val_losses = []\n    val_accs = []\n\n    if not model_save_path.exists():\n        model_save_path.mkdir(exist_ok=True, parents=True)\n\n    train_total_steps = len(train_dataloader)\n    val_total_steps = len(val_dataloader)\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss, train_accuracy = train(model, train_dataloader, device, epoch, num_epochs, train_total_steps)\n        print(\n            f'TRAINING --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n            f'Avg Loss: {train_loss}, Avg Accuracy: {train_accuracy}'\n        )\n\n        val_loss, val_accuracy = validate(model, val_dataloader, device, epoch, num_epochs, val_total_steps)\n        print(\n            f'VALIDATION --> Epoch {epoch+1}/{NUM_EPOCHS} DONE, ' +\n            f'Avg Loss: {val_loss}, Avg Accuracy: {val_accuracy}'\n        )\n\n        save_best_model(model, model_save_path, val_loss, val_losses, epoch)\n        \n        train_losses.append(train_loss)\n        train_accs.append(train_accuracy)\n        val_losses.append(val_loss)\n        val_accs.append(val_accuracy)\n    return (train_losses, train_accs), (val_losses, val_accs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:05:22.780063Z","iopub.execute_input":"2025-03-30T20:05:22.780439Z","iopub.status.idle":"2025-03-30T20:05:22.786516Z","shell.execute_reply.started":"2025-03-30T20:05:22.780407Z","shell.execute_reply":"2025-03-30T20:05:22.785609Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":216},{"cell_type":"code","source":"(train_losses, train_accs), (val_losses, val_accs) = train_loop(model, train_dataloader, val_dataloader, DEVICE, NUM_EPOCHS)\nprint(f'Best Validation Loss: {min(val_losses)} after epoch {np.argmin(val_losses) + 1}')\nprint(f'Best Validation Acc: {max(val_accs)} after epoch {np.argmax(val_accs) + 1}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:14:19.269922Z","iopub.execute_input":"2025-03-30T20:14:19.270258Z","iopub.status.idle":"2025-03-30T20:14:40.205535Z","shell.execute_reply.started":"2025-03-30T20:14:19.270228Z","shell.execute_reply":"2025-03-30T20:14:40.204479Z"}},"outputs":[{"name":"stdout","text":"Models will be saved to: models\nTRAINING --> Epoch 1/2 DONE, Avg Loss: 0.1466708933426575, Avg Accuracy: 0.9488636363636364\nVALIDATION --> Epoch 1/2 DONE, Avg Loss: 0.4452610749970464, Avg Accuracy: 0.8439171122994652\nSAVING --> First epoch: \nVal Loss: 0.4452610749970464\nSaving new model to:\nmodels/model_state_dict.pt\nTRAINING --> Epoch 2/2 DONE, Avg Loss: 0.06591293917418542, Avg Accuracy: 0.9808238636363636\nVALIDATION --> Epoch 2/2 DONE, Avg Loss: 0.5852588530191604, Avg Accuracy: 0.8146724598930482\nBest Validation Loss: 0.4452610749970464 after epoch 1\nBest Validation Acc: 0.8439171122994652 after epoch 1\n","output_type":"stream"}],"execution_count":236},{"cell_type":"code","source":"def evaluate(model, dataloader, device, total_steps):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n\n    with torch.no_grad():\n        for i,data in enumerate(dataloader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n    \n            outputs = model(ids, mask, token_type_ids)\n\n            loss = loss_fn(outputs, targets)\n            accuracy = compute_accuracy(outputs, targets)\n    \n            running_loss += loss.item()\n            running_acc += accuracy\n    \n            if (i+1) % 256 == 0:\n                print(\n                    f'TEST' +\n                    f'Step: {i+1}/{total_steps}, ' +\n                    f'Loss: {running_loss / (i+1)}, '\n                    f'Accuracy: {running_acc / (i+1)}'\n                )\n    running_loss = running_loss / total_steps\n    running_acc = running_acc / total_steps\n\n    return running_loss, running_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:20:53.479746Z","iopub.execute_input":"2025-03-30T20:20:53.480067Z","iopub.status.idle":"2025-03-30T20:20:53.485696Z","shell.execute_reply.started":"2025-03-30T20:20:53.480039Z","shell.execute_reply":"2025-03-30T20:20:53.484851Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":238},{"cell_type":"code","source":"test_loss, test_accuracy = evaluate(model, test_dataloader, DEVICE, len(test_dataloader))\nprint(\n    f'TEST --> DONE, ' +\n    f'Avg Loss: {test_loss}, Avg Accuracy: {test_accuracy}'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T20:21:07.185809Z","iopub.execute_input":"2025-03-30T20:21:07.186094Z","iopub.status.idle":"2025-03-30T20:21:07.339192Z","shell.execute_reply.started":"2025-03-30T20:21:07.186073Z","shell.execute_reply":"2025-03-30T20:21:07.338244Z"}},"outputs":[{"name":"stdout","text":"TEST --> DONE, Avg Loss: 0.4673164635896683, Avg Accuracy: 0.8802083333333334\n","output_type":"stream"}],"execution_count":240}]}